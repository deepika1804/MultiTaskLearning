{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers import GlobalAveragePooling2D, Add, concatenate, Lambda, Input, Layer, Dense, Conv2D \n",
    "from keras.layers import MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils, Sequence\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.preprocessing.image import apply_transform, transform_matrix_offset_center, flip_axis\n",
    "from keras.applications.imagenet_utils import preprocess_input, _obtain_input_shape\n",
    "from keras.initializers import he_normal\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import log_loss\n",
    "\n",
    "# from load_cifar10 import load_cifar10_data\n",
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def densenet(num_classes, input_shape):\n",
    "\n",
    "\n",
    "    def get_model(img_rows, img_cols,rgb):\n",
    "        img = Input(shape=(img_rows, img_cols, rgb))\n",
    "        A1 = NiNBlock(7, [96, 96, 96], [2,2])(img)\n",
    "        A1 = AveragePooling2D(pool_size=(3, 3),strides=(2,2),padding='same')(A1)\n",
    "    #     l1 = Dropout(0.7)(l1)\n",
    "        A1 = BatchNormalization()(A1)\n",
    "        A2 = NiNBlock(5, [256, 256, 256], [2,2])(A1)\n",
    "        A2 = AveragePooling2D(pool_size=(3, 3),strides=(2,2),padding='same')(A2)\n",
    "    #     l2 = Dropout(0.7)(l2)\n",
    "        A2 = BatchNormalization()(A2)\n",
    "        A3 = NiNBlock(3, [512, 512, 512], [1,1])(A2)\n",
    "        A3 = BatchNormalization()(A3)\n",
    "        A4 = NiNBlock(3, [1024, 1024, 512,384], [1,1])(A3)\n",
    "        A4 = BatchNormalization()(A4)\n",
    "        A5 = NiNBlock(3, [512, 512, 512], [2,2])(A4)\n",
    "        A5 = AveragePooling2D(pool_size=(3, 3),strides=(2,2),padding='same')(A5)\n",
    "        A5 = Flatten()(A5)\n",
    "        A6 = Dense(1024)(A5)\n",
    "        A6 = Activation('relu')(A6)\n",
    "        A7 = Dense(1024)(A6)\n",
    "        #add dense according to attribute layer. (for each attribute 1 or 0)\n",
    "#         l8 = Dense(1)(l7)\n",
    "        A9 = Activation('relu')(A7) # or sigmoid\n",
    "\n",
    "        model = Model(inputs=img, outputs=A9)\n",
    "        return model\n",
    "    img_rows = input_shape[0];\n",
    "    img_cols = input_shape[1];\n",
    "    rgb = input_shape[2];\n",
    "    model = get_model(img_rows,img_cols,rgb)\n",
    "    return model\n",
    "\n",
    "\n",
    "class TopSequence(Sequence):\n",
    "# class TopSequence():\n",
    "    def __init__(self, x, y, batch_size, img_size, test_mode=False):\n",
    "        print(\"init\")\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.test_mode = test_mode\n",
    "        self.data_size = x.shape[0]\n",
    "        print(self.data_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        print(\"len\")\n",
    "        return self.data_size // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    " \n",
    "    def __getitem__(self,batch_idx):\n",
    "        print(\"called\")\n",
    "        if (batch_idx + 1) * self.batch_size - 1 >= self.data_size:\n",
    "            batch_idx = np.random.randint(self.data_size - 1)\n",
    "\n",
    "        # Create empty arrays to contain batch of images and labels\n",
    "        batch_images = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], self.img_size[2]))\n",
    "        batch_top_outer_category = np.zeros((self.batch_size,5), dtype=np.uint32)\n",
    "\n",
    "        for b in range(self.batch_size):\n",
    "            img_path = self.x[batch_idx * self.batch_size + b]\n",
    "            im = cv2.imread(img_path)\n",
    "            im = cv2.resize(im,(self.img_size[0], self.img_size[1]))\n",
    "            im = im / 255.0\n",
    "            cv2.imshow(im)\n",
    "#             im = augment_image(im)\n",
    "#             im = im * 255.0\n",
    "            batch_images[b, :, :, :] = im                                                    #storing image\n",
    "            batch_top_outer_category[b, :] = np_utils.to_categorical(int(self.y[batch_idx * self.batch_size + b]), 5)\n",
    "        \n",
    "        batch_images = preprocess_input(batch_images)\n",
    "        print(batch_images.shape)\n",
    "        return batch_images, batch_top_outer_category\n",
    "\n",
    "\n",
    "img_rows, img_cols = 112, 112\n",
    "channel = 3\n",
    "num_classes = 18\n",
    "batch_size = 100\n",
    "nb_epoch = 150\n",
    "\n",
    "X_train = []\n",
    "X_valid = []\n",
    "Y_train = []\n",
    "Y_valid = []\n",
    "\n",
    "with open(\"top_outer_category_balanced_train.csv\") as file_obj:\n",
    "    reader = csv.DictReader(file_obj, delimiter=',')\n",
    "    for line in reader:\n",
    "        X_train.append(line['x'])\n",
    "        Y_train.append(line['y'])\n",
    "\n",
    "with open(\"top_outer_category_balanced_test.csv\") as file_obj:\n",
    "    reader = csv.DictReader(file_obj, delimiter=',')\n",
    "    for line in reader:\n",
    "        X_valid.append(line['x'])\n",
    "        Y_valid.append(line['y'])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_valid = np.array(X_valid)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_valid = np.array(Y_valid)\n",
    "\n",
    "model = densenet(num_classes, input_shape=(112,112,3))\n",
    "\n",
    "top_outter_category_layer = Dense(5, activation='softmax', name='top_outer_category_pred')(model.layers[-1].output)\n",
    "\n",
    "model = Model(model.input,top_outter_category_layer,name=\"final\")\n",
    "print(model.summary())\n",
    "\n",
    "#opt = SGD(lr=0.001, momentum=0.9, decay=0.0005)\n",
    "opt = Adam(lr = 0.001,decay=0.0005)\n",
    "model.compile(loss=['categorical_crossentropy'],optimizer=opt,metrics=['accuracy'])\n",
    "path = 'model_top_category_p1_p25.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=path,verbose=1,save_best_only=True)\n",
    "#model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
    "\n",
    "train_seq = TopSequence(X_train[0:5], Y_train[0:5], batch_size, img_size=[112,112,3])\n",
    "val_seq = TopSequence(X_valid[0:5], Y_valid[0:5], batch_size, img_size=[112,112,3])\n",
    "\n",
    "model.fit_generator(\n",
    "        train_seq,\n",
    "        steps_per_epoch=len(train_seq),\n",
    "        epochs=nb_epoch,\n",
    "        callbacks=[checkpoint],\n",
    "        validation_data=val_seq,\n",
    "        validation_steps=len(val_seq),\n",
    "        max_queue_size= 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
